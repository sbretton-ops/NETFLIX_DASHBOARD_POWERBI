{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b67533bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import os\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c91112d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9ae220a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Récupération de la clé depuis l'environnement \n",
    "api_key_env = os.environ.get(\"OMDB_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a4a35b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEYS = [api_key_env] if api_key_env else []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a49b3bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage du processus OMDb avec une limite de 9000 requêtes/session (avec 1 clé active)...\n",
      "Cache initialisé : Première exécution.\n",
      "Titres restants à traiter: 8793\n",
      "Début du traitement de 8793 titres pour cette session.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8793/8793 [2:18:41<00:00,  1.06it/s]     "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Session Terminée ---\n",
      "8793 nouvelles lignes traitées.\n",
      "Fichier de résultat (cache) enregistré : imdb_scores_export.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# --- 1. Configuration (Clé et Limites) ---\n",
    "DATA_FILE = \"titles_netflix.csv\"\n",
    "OUTPUT_FILE = \"imdb_scores_export.csv\" \n",
    "\n",
    "\n",
    "MAX_REQUESTS_PER_KEY = 9000\n",
    "MAX_REQUESTS = 9000\n",
    "BASE_URL = \"http://www.omdbapi.com/\"\n",
    "\n",
    "# Vérification de la clé API\n",
    "if not API_KEYS:\n",
    "    print(\"ERREUR CRITIQUE: Aucune clé API valide trouvée. Le script ne peut pas s'exécuter.\")\n",
    "    exit() \n",
    "\n",
    "\n",
    "# --- 2. Fonction d'Extraction (Titre + Année) - \n",
    "def fetch_omdb_data(title, year, api_key):\n",
    "    \"\"\"\n",
    "    Appelle l'API OMDb pour récupérer l'ID IMDb et la Note IMDb.\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        't': title,\n",
    "        'y': year, \n",
    "        # CORRECTION : 'tomatoes': 'true' est supprimé\n",
    "        'apikey': api_key\n",
    "    }\n",
    "    \n",
    "    params = {k: v for k, v in params.items() if v} \n",
    "\n",
    "    try:\n",
    "        response = requests.get(BASE_URL, params=params)\n",
    "        response.raise_for_status() \n",
    "        data = response.json()\n",
    "        \n",
    "        # Initialisation des résultats (Utilisation de imdb_score)\n",
    "        results = {\n",
    "            'title': title, \n",
    "            'release_year': year,\n",
    "            'imdb_id': 'N/A', \n",
    "            'imdb_score': 'N/A' \n",
    "        }\n",
    "        \n",
    "        if data.get('Response') == 'True':\n",
    "            results['imdb_id'] = data.get('imdbID', 'N/A')\n",
    "            results['imdb_score'] = data.get('imdbRating', 'N/A')\n",
    "            \n",
    "            return results\n",
    "                \n",
    "        # Le film n'a pas été trouvé (Response: 'False')\n",
    "        else:\n",
    "            return {'title': title, 'release_year': year, 'imdb_id': 'N/A_Not_Found', 'imdb_score': 'N/A_Not_Found'}\n",
    "            \n",
    "    except requests.exceptions.RequestException as e:\n",
    "        return {'title': title, 'release_year': year, 'imdb_id': 'ERROR_API', 'imdb_score': 'ERROR_API'}\n",
    "\n",
    "\n",
    "# --- 3. Logique de Caching et de Traitement ---\n",
    "\n",
    "def run_single_phase_process():\n",
    "    cache_columns = ['title', 'release_year', 'imdb_id', 'imdb_score']\n",
    "    \n",
    "    print(f\"Démarrage du processus OMDb avec une limite de {MAX_REQUESTS} requêtes/session (avec {len(API_KEYS)} clé active)...\")\n",
    "    \n",
    "    # Chargement du DataFrame Netflix\n",
    "    try:\n",
    "        df_netflix = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERREUR: Fichier d'entrée non trouvé: {DATA_FILE}. Veuillez vérifier le nom du fichier.\")\n",
    "        return\n",
    "\n",
    "    # Vérification et Préparation des colonnes\n",
    "    if 'title' not in df_netflix.columns or 'release_year' not in df_netflix.columns:\n",
    "        print(\"ERREUR: Les colonnes 'title' et/ou 'release_year' sont manquantes.\")\n",
    "        return\n",
    "        \n",
    "    df_netflix['release_year'] = df_netflix['release_year'].astype(str).str.replace(r'\\.0$', '', regex=True)\n",
    "\n",
    "    # Chargement ou initialisation du Cache \n",
    "    if os.path.exists(OUTPUT_FILE):\n",
    "        try:\n",
    "            df_cache = pd.read_csv(OUTPUT_FILE)\n",
    "            if 'imdb_score' in df_cache.columns:\n",
    "                 # Filtre les lignes déjà traitées avec succès\n",
    "                 df_cache = df_cache[~df_cache['imdb_score'].isin(['ERROR_API', 'N/A_Not_Found'])]\n",
    "                 df_cache = df_cache[[c for c in cache_columns if c in df_cache.columns]]\n",
    "            else:\n",
    "                 df_cache = pd.DataFrame(columns=cache_columns)\n",
    "                 \n",
    "        except Exception:\n",
    "            df_cache = pd.DataFrame(columns=cache_columns)\n",
    "            print(\" Erreur de lecture du cache, réinitialisation du cache.\")\n",
    "            \n",
    "        print(f\" Cache chargé: {len(df_cache)} titres déjà traités avec succès.\")\n",
    "    else:\n",
    "        df_cache = pd.DataFrame(columns=cache_columns)\n",
    "        print(\"Cache initialisé : Première exécution.\")\n",
    "\n",
    "    # Créer une clé unique pour le filtrage (Titre + Année)\n",
    "    df_netflix['unique_key'] = df_netflix['title'].str.cat(df_netflix['release_year'], sep='|')\n",
    "    df_cache['unique_key'] = df_cache['title'].str.cat(df_cache['release_year'].astype(str), sep='|')\n",
    "    \n",
    "    # Identifier les titres restants à traiter\n",
    "    merged_df = pd.merge(df_netflix, df_cache, on='unique_key', how='left', suffixes=('_netf', '_cache'))\n",
    "    \n",
    "    # On ne garde que les lignes qui n'ont pas encore de score IMDb (colonne créée par le merge)\n",
    "    df_to_process = merged_df[merged_df['imdb_score'].isna()] \n",
    "    \n",
    "    print(f\"Titres restants à traiter: {len(df_to_process)}\")\n",
    "\n",
    "    # Appliquer la limite de la session\n",
    "    df_to_process_session = df_to_process.head(MAX_REQUESTS)\n",
    "    print(f\"Début du traitement de {len(df_to_process_session)} titres pour cette session.\")\n",
    "\n",
    "    new_results = []\n",
    "    processed_count = 0\n",
    "    \n",
    "    # Boucle de traitement\n",
    "    for index, row in tqdm(df_to_process_session.iterrows(), total=len(df_to_process_session)):\n",
    "        \n",
    "        # NOTE : Il n'y a plus de rotation, on utilise la seule clé valide [0]\n",
    "        current_api_key = API_KEYS[0] \n",
    "        \n",
    "        result = fetch_omdb_data(row['title_netf'], row['release_year_netf'], current_api_key)\n",
    "        \n",
    "        new_results.append(result)\n",
    "        processed_count += 1\n",
    "        \n",
    "        time.sleep(0.5) # Pause de sécurité\n",
    "\n",
    "    # 4. Enregistrer les résultats (Mise à jour du Cache)\n",
    "    if new_results:\n",
    "        df_new_results = pd.DataFrame(new_results)\n",
    "        \n",
    "        df_combined = pd.concat([df_cache.drop(columns='unique_key', errors='ignore'), df_new_results])\n",
    "        df_combined = df_combined.drop_duplicates(subset=['title', 'release_year'], keep='last')\n",
    "        \n",
    "        df_combined.to_csv(OUTPUT_FILE, index=False)\n",
    "        \n",
    "        print(\"\\n--- Session Terminée ---\")\n",
    "        print(f\"{processed_count} nouvelles lignes traitées.\")\n",
    "        print(f\"Fichier de résultat (cache) enregistré : {OUTPUT_FILE}\")\n",
    "    else:\n",
    "        print(\"Aucun nouveau titre traité. Le cache est à jour pour cette limite.\")\n",
    "\n",
    "\n",
    "# --- Lancement du Processus ---\n",
    "if __name__ == \"__main__\":\n",
    "    run_single_phase_process()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
